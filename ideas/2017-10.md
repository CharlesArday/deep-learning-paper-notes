## 10/5

Does it make sense to visualize how smooth a gradient is? From "Deep Conv NN Design Patterns":
> A recent paper (Alain & Bengio (2016)) showed in an experiment that using an identity skip length of 64 in a network of depth 128 led to the first portion of the network not being trained
Which a gradient visualizer might have shown. Suppose I should check out TensorBoard.

## 10/4

NMT: deep (8-layer) encoders with residual connections diverge 3 out of 4 times, according to MEONMTA, leading to a std dev in BLEU of 2.64. Can we make this more robust? What research has been done into RNN initializations?

## 10/3

#### Long sequences

Find a task for which attention performs poorly but shouldn't and try to visualize what the attentional vector is visualizing.

Mechanisms for changing attention weight vectors:

* Might it be helpful to distort attention weights with activation functions of some kind (squishing them down so they're mostly zeroes, for example, or "log-loss"ing them so that highly confident guesses are downgraded?)
* Alternatively, does it make any sense to add some kind of penalty term for attentional weights? This could also shape the weights in some way.

For super long sequences, does attention hit any limits? I know RNNs do. And I know the idea of hard/local attention has been tried - the Attention Is All You Need authors definitely cited these examples as bad, because they still scaled with the length of the input sequence somehow. How long are the longest sequences that researchers generally work with? Could a model summarize entire books?

Model summarizing entire books: can this be parallelized by e.g., chapter, and then representations added/concated together? Maybe I should read more about summarization. (How do I find the best recent papers?)

#### Attention vs RNNs

Is it really true that it's an "open secret" that RNNs might get replaced by attention? The Attention Is All You Need paper was just seeking to eliminate the sequentialness constraint of RNNs so that they could parallelize processing of a long individual sequences (imagine summarizing a book and doing a sequential pass for each of the 1M words).

* Read [Training RNNs as Fast as CNNs](https://arxiv.org/abs/1709.02755v2)
* Look for research on really long sequences?
